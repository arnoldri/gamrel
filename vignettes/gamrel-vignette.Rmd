---
title: "Non-parametric GaPP"
output: bookdown::html_document2
link-citations: yes
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{gamrel-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: nonparametric-bib.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(gamrel)
```

# Background

A first attempt was written up in @Arnold_Chukova_Hayakawa_APARM:2020.

## Errata: 

  * In eq. (17) $\alpha_1$ should be $\alpha$ [R code was correct]
  * In eq. (19) the exponent $a_1+K$ should be $a_1+K-1$ [R code was incorrect, but this effectively changed the value of $a_1$ in the hyperprior specification]
  * In eq. (19) the expression $\log w_K$ should be $\log \tilde{w}_K$ [R code was correct]
  * In the description of the simulation process in \Sec IV the locations (and their associated weigts) need to be sorted so that the locations $\theta_k$ are in ascending order [R code was correct]

# Model

## Basic Model Specification

Times to failure are drawn from an inhomogenous hazard rate function $\lambda(t)$.  Failures are iid, with density $f(t|\lambda)$ and survival function $\bar{F}(t|\lambda)$
\begin{eqnarray}
   f(t | \lambda) &=& \lambda(t) e^{-\Lambda(t)}    
   (\#eq:density)\\
   \bar{F}(t | \lambda) &=& e^{-\Lambda(t)}
   (\#eq:survival-function)
\end{eqnarray}
where
\begin{equation}
   \Lambda(t) = \int_0^t \lambda(u)\,{\rm d}u  
   (\#eq:cumulative-hazard)
\end{equation}
is (non-decreasing) the cumulative hazard function.

If observations are censored at time $\tau$, then the likelihood of $n$ observations, $n_0$ observed and $n-n_0$ censored, is
\begin{equation}
   f(\{t_i\}_{i=1}^n | \lambda) = \left[\prod_{i=1}^{n_0} \lambda(t_i) \right]
                                  e^{-\sum_{i=1}^{n_0} \Lambda(t_i) - (n-n_0)\Lambda(\tau)}
   (\#eq:model-likef)
\end{equation}

## Simulation

To Draw $n$ observations from the model according to \@ref(eq:model-likef): 

  * Draw $n$ iid values $\{u_i\}_{i=1}^n$ from $\text{Uniform}(0,1)$
  * For each $i=1,\ldots,n$ solve
    \[
        \Lambda(t^\ast_i) = -\log u_i
    \]
  * Set $t_i = \text{min}(t^\ast_i,\tau)$

## Priors on the hazard rate function

Let $G(\cdot)$, $G_1(\cdot)$ and $G_2(\cdot)$ be non-normalised measures with support on the positive real line.  Then consider four cases:

  1. Increasing Failure Rate (IFR) (After @LoWeng:1989)
    \begin{equation}
       \lambda(t|G) = \lambda_0 + \int_0^t G({\rm d}u)
       (\#eq:ifr)
    \end{equation}
  1. Decreasing Failure Rate (DFR) (After @LoWeng:1989)
    \begin{equation}
       \lambda(t|G) = \lambda_0 + \int_t^\infty G({\rm d}u)
       (\#eq:dfr)
    \end{equation}
  1. Lo-Weng Bathtub (LWB) (After @LoWeng:1989)
    \begin{equation}
       \lambda(t|G) = \lambda_0 + \int_0^{|t-a|} G({\rm d}u)
       (\#eq:low-weng-bathtub)
    \end{equation}
    for constant $a>0$.
  1. Superposition Bathtub (SBT) (After @Arnold_Chukova_Hayakawa_APARM:2020)
    \begin{equation}
       \lambda(t|\lambda_0, G_1, G_2) = \lambda_0 + \int_t^\infty G_1({\rm d}u) + \int_0^t G_2({\rm d}u)
       (\#eq:superposition-bathtub)
    \end{equation}
    Here $G_1$ is DFR (determines the early behaviour) and $G_2$ is IFR (determines the late behaviour).
  1. Mixture Bathtub (MBT): Specify through the survival function
    \begin{equation}
       \bar{F}(t|\lambda(\cdot|\pi, G_1, G_2)) = \pi \bar{F}_{\rm DFR}(t|G_1) + (1-\pi) \bar{F}_{\rm IFR}(t|G_2)
       (\#eq:mixture-bathtub)
    \end{equation}
    Where $0<\pi<1$ and $\bar{F}_{\rm DFR}(t|G)$ and $\bar{F}_{\rm IFR}(t|G)$ are the respective survival functions 
    from the IFR and DFR cases defined above. 
  1. Log-convex (LCV)
    \begin{equation}
       \frac{d\log\lambda(t|G)}{dt} = w_0 + \int_0^t G({\rm d}u)
    \end{equation}
    which implies that
    \begin{equation}
       \lambda(t|G) = \lambda_0 e^{w_0t + \int_0^t (t-u) G({\rm d}u)}
       (\#eq:lcfr)
    \end{equation}
    Here the constant $w_0\geq 0$ gives an IFR distribution, but $w_0<0$ gives a bathtub.

Gamma Process Priors can be placed on the mixing measures $G$, $G_1$ and $G_2$ and implemented under the stick-breaking convention.   Specifically if $G|\alpha H_0,\beta \sim \text{GaPP}(\alpha H_0,\beta)$ for probability measure $H_0$ (with support on the positive real line) and postive constants $\alpha$, $\beta$, then a draw from the prior can be written
\begin{eqnarray}
   \theta_k | H_0 &\overset{\rm iid}{\sim}& H_0 \qquad \text{for $k=1,2,\ldots$}\\
   v_k | \alpha &\overset{\rm iid}{\sim}& \text{Be}(1,\alpha)\\
   \tilde{w}_k &=& v_k \prod_{\ell<k} (1-v_\ell)\\
   \gamma &\sim& \text{Ga}(\alpha,\beta)\\
   G({\rm d}u) &=& \gamma\sum_{k=1}^\infty \tilde{w}_k \delta_{\theta_k}({\rm d}u)
   (\#eq:gsum-inf)
\end{eqnarray}
so that
\begin{eqnarray}
  \int_0^t G({\rm d}u) &=& \gamma \sum_{k=1}^\infty \tilde{w}_k I(\theta_k<t)\\
  \int_t^\infty G({\rm d}u) &=& \gamma \sum_{k=1}^\infty \tilde{w}_k I(\theta_k>t)
\end{eqnarray}
and
\begin{eqnarray}
  \int_0^t \int_0^u G({\rm d}v) &=& \gamma \sum_{k=1}^\infty \tilde{w}_k \text{max}(t-\theta_k,0)\\
  \int_0^t \int_u^\infty G({\rm d}v) &=& \gamma \sum_{k=1}^\infty \tilde{w}_k \text{min}(t,\theta_k)
\end{eqnarray}

The weights $\tilde{w}_k$ are stochastically decreasing in $k$, which means that it is possible to truncate the sum in 
eq. \@ref(eq:gsum-inf) after $K$ terms.  In that case we have
\begin{eqnarray}
   \theta_k | H_0 &\overset{\rm iid}{\sim}& H_0 \qquad \text{for $k=1,2,\ldots,K$}\\
   v_k | \alpha &\overset{\rm iid}{\sim}& \text{Be}(1,\alpha) \qquad \text{for $k=1,2,\ldots,K-1$}\\
   \tilde{w}_k &=& v_k \prod_{\ell<k} (1-v_\ell)  \qquad \text{for $k=1,2,\ldots,K-1$}\\
   \tilde{w}_K &=& 1-\sum_{k=1}^{K-1} \tilde{w}_k = \prod_{\ell=1}^{K-1} (1-v_\ell)\\
   \gamma &\sim& \text{Ga}(\alpha,\beta)\\
   G({\rm d}u) &=& \gamma\sum_{k=1}^K \tilde{w}_k \delta_{\theta_k}({\rm d}u)
   (\#eq:gsum-inf)
\end{eqnarray}
A fully hierarchical Bayesian model for $G$ can be completed by choosing a form for $H_0$, with parameters $\phi$, and specifying suitable priors for $\alpha$, $\beta$, $\gamma$ and $\phi$.  For example if $H_0=\text{Exp}(\phi)$ then
\begin{eqnarray}
   \alpha &\sim& \text{Ga}(a_1,a_2)\\
   \beta  &\sim& \text{Ga}(b_1,b_2)\\
   \phi   &\sim& \text{Ga}(f_1,f_2)
\end{eqnarray}

## Specific hazard rate functions

In what follows we write Write $w_k = \gamma\tilde{w}_k$, and also 
define $k^\ast(t) = \text{max}\{k\in (1,\ldots,K)\,:\,\theta^\ast_k\leq t\}$ which implies that
\begin{equation}
    \theta^\ast_{k^\ast(t)} \leq t < \theta^\ast_{k^\ast(t)+1}
\end{equation}
And we may need to re-index $\{(w_k,\theta_k)\}_{k=1}^K$ as $\{(w^\ast_k,\theta_k^\ast)\}_{k=1}^K$ such that
\[
    \theta^\ast_0 \equiv 0 < \theta^\ast_1 < \theta^\ast_2 < \ldots < \theta^\ast_K
\]
(Note that the $\theta_k$ values are almost surely distinct.)  

We also define the following partial sums of the ordered weights and locations:
\begin{eqnarray}
   C^\ast_\ell &=& \sum_{k=1}^\ell w^\ast_k = \gamma - \sum_{k=\ell+1}^K w_k^\ast\\
   D^\ast_\ell &=& \sum_{k=1}^\ell w^\ast_k \theta^\ast_k
\end{eqnarray}

### IFR case

\begin{eqnarray}
   \lambda(t|\lambda_0,\gamma,\mathbf{\theta},\mathbf{v})
      &=& \lambda_0 + \gamma \sum_{k=1}^K \tilde{w}_k I(\theta_k\leq t)\\
   \Lambda(t|\lambda_0,\gamma,\mathbf{\theta},\mathbf{v})
      &=& \lambda_0 t + \gamma \sum_{k=1}^K \tilde{w}_k \text{max}(0,t-\theta_k)
\end{eqnarray}
If we define $\Lambda^\ast_k=\Lambda(\theta^\ast_k)$ with $\theta^\ast_0=0$ and $\Lambda^\ast_0=0$, then $\Lambda$ is piecewise linear between the points $\{(\theta^\ast_k,\Lambda^\ast_k)\}_{k=0}^K$.   If $U\sim\text{Uniform}(0,1)$ then $T$ is a draw from this distribution if
\begin{eqnarray}
   T = \frac{-\log U + D^\ast_{k^\ast}}{\lambda_0 + C^\ast_{k^\ast}}
\end{eqnarray}
where $k^\ast = \text{max}\{k\in\{1,\ldots,K\}\,:\,\Lambda^\ast_{k}\leq -\log U\}$.

The prior includes the prior for $G\sim\text{GaPP}(\alpha H_0,\beta)$ and a suitable prior for 
$\lambda_0|G$:
\[
   \frac{\lambda_0}{\gamma} \sim \text{Exp}(\nu)
\]

### DFR case

\begin{eqnarray}
   \lambda(t|\lambda_0,\gamma,\mathbf{\theta},\mathbf{v})
      &=& \lambda_0 + \gamma \sum_{k=1}^K \tilde{w}_k I(\theta_k>t)\\
   \Lambda(t|\lambda_0,\gamma,\mathbf{\theta},\mathbf{v})
      &=& \lambda_0 t + \gamma \sum_{k=1}^K \tilde{w}_k \text{min}(t,\theta_k)
\end{eqnarray}
Following the same protocol as in the IFR case: 
if we define $\Lambda^\ast_k=\Lambda(\theta^\ast_k)$ with $\theta^\ast_0=0$ and $\Lambda^\ast_0=0$, then $\Lambda$ is piecewise linear between the points $\{(\theta^\ast_k,\Lambda^\ast_k)\}_{k=0}^K$.   If $U\sim\text{Uniform}(0,1)$ then $T$ is a draw from this distribution if
\begin{eqnarray}
   T = \frac{-\log U - D^\ast_{k^\ast}}{\lambda_0 + \gamma - C^\ast_{k^\ast}}
\end{eqnarray}
where $k^\ast = \text{max}\{k\in\{1,\ldots,K\}\,:\,\Lambda^\ast_{k}\leq -\log U\}$

As in the DFR case, the prior includes the prior for $G\sim\text{GaPP}(\alpha H_0,\beta)$ and a 
suitable prior for $\lambda_0|G$:
\[
   \frac{\lambda_0}{\gamma} \sim \text{Exp}(\nu)
\]

### LWB case

\begin{eqnarray}
   \lambda(t|\lambda_0,a,\gamma,\mathbf{\theta},\mathbf{v})
      &=& \lambda_0 + \gamma \sum_{k=1}^K \tilde{w}_k I(0<\theta_k<|t-a|)\\
      &=& \lambda_0 + 
          \begin{cases}
          \gamma\sum_{k=1}^K \tilde{w}_k I(t<a-\theta_k)\;\; & \text{if $t<a$}\\
          \gamma\sum_{k=1}^K \tilde{w}_k I(t\geq a+\theta_k)\;\; & \text{if $t\geq a$}
          \end{cases}\\
   \Lambda(t|\lambda_0,a,\gamma,\mathbf{\theta},\mathbf{v})
      &=& \lambda_0 t + 
          \begin{cases}
          \gamma\sum_{k=1}^K \tilde{w}_k I(\theta_k<a)\text{min}(t,a-\theta_k)\;\; 
                          & \text{if $t<a$}\\
          \gamma\sum_{k=1}^K \tilde{w}_k 
          \left[\text{max}(0,a-\theta_k)+\text{max}(t-a-\theta_k,0)\right]\;\; & \text{if $t\geq a$}
          \end{cases}
\end{eqnarray}
Combine the $\{(\theta_{k},w_k)\}_{k=1}^K$ values with one further location, weight pair: $(a,0)$.  Order the locations, and their associated weights, forming the set $\{(w^{\ast\ast}_k,\theta^{\ast\ast}_k)\}_{k=1}^{K+1}$.  Compute the cumulative hazard function values $\Lambda^{\ast\ast}_k=\Lambda(\theta^{\ast\ast}_k)$.  Then if $U\sim\text{Uniform}(0,1)$ then $T$ is a draw from this distribution if
\begin{eqnarray}
   T = \theta^{\ast\ast}_{k^\ast} 
          + \frac{\theta^{\ast\ast}_{k^\ast+1}-\theta^{\ast\ast}_{k^\ast}}{
                                   \Lambda^{\ast\ast}_{k^\ast+1}-\Lambda^{\ast\ast}_{k^\ast}}
                                   \left(-\log U-\Lambda^{\ast\ast}_{k^\ast}\right)
\end{eqnarray}
where $k^\ast = \text{max}\{k\in\{1,\ldots,K_1+K_2\}\,:\,\Lambda^{\ast\ast}_{k}\leq -\log U\}$

Once again the prior includes the prior for $G\sim\text{GaPP}(\alpha H_0,\beta)$ and a suitable prior for $\lambda_0|G$:
\[
   \frac{\lambda_0}{\gamma} \sim \text{Exp}(\nu)
\]


### SBT case

Replicate the prior structure for $G$ for each of $G_1$ and $G_2$, 
indexing the parameters for each component with the label $c=1,2$, 
and add a prior for $\lambda_0|G_2$:
\begin{equation}
    \frac{\lambda_0}{\gamma_2} \sim \text{Exp}(\nu)
\end{equation}

\begin{eqnarray}
   \lambda(t|\lambda_0,\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1,\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)
      &=& \lambda_0 + \gamma_1 \sum_{k=1}^{K_1} \tilde{w}_{1k} I(\theta_{1k}>t)
                    + \gamma_2 \sum_{k=1}^{K_2} \tilde{w}_{2k} I(\theta_{2k}\leq t)\\
   \Lambda(t|\lambda_0,\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1,\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)
      &=& \lambda_0 t + \gamma_1 \sum_{k=1}^{K_1} \tilde{w}_{1k} \text{min}(t,\theta_{1k})
                      + \gamma_2 \sum_{k=1}^{K_2} \tilde{w}_{2k} \text{max}(0,t-\theta_{2k})
\end{eqnarray}
Extend the IFR/DFR approach: pool the $\theta_{1k}$ and $\theta_{2k}$ locations and order them, and their associated weights, forming the set $\{(w^{\ast\ast}_k,\theta^{\ast\ast}_k)\}_{k=1}^{K_1+K_2}$.  Compute the cumulative hazard function values $\Lambda^{\ast\ast}_k=\Lambda(\theta^{\ast\ast}_k)$.  Then if $U\sim\text{Uniform}(0,1)$ then $T$ is a draw from this distribution if
\begin{eqnarray}
   T &=& \theta^{\ast\ast}_{k^\ast} 
         + \frac{\theta^{\ast\ast}_{k^\ast+1}-\theta^{\ast\ast}_{k^\ast}}{
                                   \Lambda^{\ast\ast}_{k^\ast+1}-\Lambda^{\ast\ast}_{k^\ast}}
                                   \left(-\log U-\Lambda^{\ast\ast}_{k^\ast}\right)
\end{eqnarray}
where $k^\ast = \text{max}\{k\in\{1,\ldots,K_1+K_2\}\,:\,\Lambda^{\ast\ast}_{k}\leq -\log U\}$

### MBT case

\begin{eqnarray}
   \lambda(t|\pi,
            \lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1,
            \lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)
   &=& \frac{f(t|\pi,
                  \lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1,
                  \lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)}{
             \bar{F}(t|\pi,
                  \lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1,
                  \lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)}\\
   &=& \frac{\pi \lambda_{\rm DFR}(t| \lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1) 
             \bar{F}_{\rm DFR}(t| \lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1)
             +(1-\pi) \lambda_{\rm IFR}(t| \lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2) 
             \bar{F}_{\rm IFR}(t|\lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)
   }{\pi \bar{F}_{\rm DFR}(t|\lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1)
             +(1-\pi) \bar{F}_{\rm IFR}(t|\lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)}\\
   \Lambda(t|\pi,\lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1,
                 \lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)
   &=& -\log(\bar{F}(t|\pi,\lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1,
                           \lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2))\\
   &=& -\log\left(\pi \bar{F}_{\rm DFR}(t|\lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1)
            +(1-\pi) \bar{F}_{\rm IFR}(t|\lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)\right)\\
   &=& -\log\left(\pi e^{-\Lambda_{\rm DFR}(t|\lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1)}
             +(1-\pi) 
             e^{-\Lambda_{\rm IFR}(t|\lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)}\right)
\end{eqnarray}
Simulate as a mixture: draw $c\sim\text{Cat}(\{1,2\};(\pi,1-\pi))$ then if $c=1$ draw from the DFR component (parameters $\lambda_{01},\gamma_1,\mathbf{\theta}_1,\mathbf{v}_1)$), and if $c=2$ draw from the IFR component (parameters $\lambda_{02},\gamma_2,\mathbf{\theta}_2,\mathbf{v}_2)$).

The prior includes the priors for 
$G_1\sim\text{GaPP}(\alpha_1 H_{10},\beta_1)$ 
and $G_2\sim\text{GaPP}(\alpha_2 H_{20},\beta_2)$ 
and suitable priors for $\lambda_{10}|G_1$ and $\lambda_{20}|G_2$ are:
\begin{eqnarray}
   \frac{\lambda_{10}}{\gamma_1} &\sim& \text{Exp}(\nu)\\
   \frac{\lambda_{20}}{\gamma_2} &\sim& \text{Exp}(\nu)
\end{eqnarray}

### Log convex case

\begin{eqnarray}
   \frac{d\log\lambda(t|\lambda_0,w_0,\gamma,\mathbf{\theta},\mathbf{v})}{dt} 
   &=& w_0 + \sum_{k=1}^K w_k I(\theta_k\geq t)\\
   \log\lambda(t|\lambda_0,w_0,\gamma,\mathbf{\theta},\mathbf{v}) &=& \log\lambda_0 + w_0 t 
                                           + \sum_{k=1}^K w_k \text{max}(0,t-\theta_k)\\
   \lambda(t|\lambda_0,w_0,\gamma,\mathbf{\theta},\mathbf{v}) &=& \lambda_0 \exp\left(w_0 t + \sum_{k=1}^K w_k \text{max}(0,t-\theta_k)\right)\\
\end{eqnarray}
Then, switching to the re-indexed set $\{(w^\ast_k,\theta^\ast_k)\}_{k=1}^K$
\begin{eqnarray}
   \Lambda(t|\lambda_0,w_0,\gamma,\mathbf{\theta},\mathbf{v}) &=& \sum_{\ell=0}^{k^\ast(t)-1} 
                      \int_{\theta^\ast_\ell}^{\theta^\ast_{\ell+1}} 
                            \lambda(u|\lambda_0,w_0,\gamma,\mathbf{\theta},\mathbf{v})\,{\rm d}u\\
                  & & + \int_{\theta^\ast_{k^\ast(t)}}^{t} 
                            \lambda(u|\lambda_0,w_0,\gamma,\mathbf{\theta},\mathbf{v})\,{\rm d}u\\
                  &=& \sum_{\ell=0}^{k^\ast(t)-1} 
                      \int_{\theta^\ast_\ell}^{\theta^\ast_{\ell+1}} 
                            \lambda_0\exp\left(w_0 u + \sum_{k=1}^K w^\ast_k \text{max}(0,u-\theta^\ast_k)\right)
                            \,{\rm d}u\\
                  & & + \int_{\theta^\ast_{k^\ast(t)}}^{t} 
                            \lambda_0\exp\left(w_0 u + \sum_{k=1}^K w^\ast_k \text{max}(0,u-\theta^\ast_k)\right)
                            \,{\rm d}u\\
                  &=& \sum_{\ell=0}^{k^\ast(t)-1} 
                      \int_{\theta^\ast_\ell}^{\theta^\ast_{\ell+1}} 
                            \lambda_0\exp\left(w_0 u + \sum_{k=1}^\ell w^\ast_k (u-\theta^\ast_k)\right)
                            \,{\rm d}u\\
                  & & + \int_{\theta^\ast_{k^\ast(t)}}^{t} 
                            \lambda_0\exp\left(w_0 u + \sum_{k=1}^{k^\ast(t)} w^\ast_k (u-\theta^\ast_k)\right)
                            \,{\rm d}u\\
                  &=& \sum_{\ell=0}^{k^\ast(t)-1} 
                      \int_{\theta^\ast_\ell}^{\theta^\ast_{\ell+1}} 
                            \lambda_0 e^{C^\ast_{0\ell} u - D^\ast_\ell}
                            \,{\rm d}u\\
                  & & + \int_{\theta^\ast_{k^\ast(t)}}^{t} 
                            \lambda_0 e^{C^\ast_{0k^\ast(t)} - D^\ast_{k^\ast(t)}}
                            \,{\rm d}u\\
                  &=& \sum_{\ell=0}^{k^\ast(t)-1} \lambda_0 e^{-D^\ast_\ell}
                      \left\{\frac{e^{C^\ast_{0\ell}\theta^\ast_{\ell+1}}
                            -e^{C^\ast_{0\ell}\theta^\ast_{\ell}}}{C^\ast_{0\ell}}\right\}\\
                  & & + \lambda_0 e^{- D^\ast_{k^\ast(t)}}
                      \left\{\frac{e^{C^\ast_{0k^\ast(t)}t}
                          -e^{C^\ast_{0k^\ast(t)}\theta^\ast_{k^\ast(t)}}}{C^\ast_{0k^\ast(t)}}\right\}
\end{eqnarray}
where
\begin{eqnarray}
   C^\ast_{0\ell} = w_0 + C^\ast_\ell = \sum_{k=0}^\ell w^\ast_k
\end{eqnarray}
Note that $C^\ast_{0k}$ is non-decreasing in $k$, and in principle can take zero values.  Where
$C^\ast_{0k}$ is zero the quantities in the braces in the expression for $\Lambda(t|\lambda_0,w_0,\gamma,\theta,{\bf v})$ should be replaced by
\[
    \theta^\ast_{k+1}-\theta^\ast_{k}
\]
or
\[
    t - \theta^\ast_{k^\ast(t)}
\]

Again following the same protocol as in the IFR case
we define $\Lambda^\ast_k=\Lambda(\theta^\ast_k)$ with $\theta^\ast_0=0$ and $\Lambda^\ast_0=0$.  Then in the interval
$\theta^\ast_{k^\ast(t)}\leq t < \theta^\ast_{k^\ast(t)}$ the function $\Lambda$ has the exponential form
\begin{eqnarray}
   \Lambda(t|\lambda_0,w_0,\gamma,\mathbf{\theta},\mathbf{v})
   &=&
   \Lambda^\ast_{k^\ast(t)} + \frac{\lambda_0 e^{- D^\ast_{k^\ast(t)}}}{C^\ast_{0l^\ast}}
                      \left\{e^{C^\ast_{0k^\ast(t)}t}
                           -e^{C^\ast_{0k^\ast(t)}\theta^\ast_{k^\ast(t)}}\right\}
\end{eqnarray}
It follows that 
If $U\sim\text{Uniform}(0,1)$ then $T$ is a draw from this distribution if
\begin{eqnarray}
   T = \frac{1}{C^\ast_{0k^\ast}}\log\left[
         e^{C^\ast_{0k^\ast}\theta^\ast_{k^\ast}}
       + C^\ast_{0k^\ast}\frac{-\log U - \Lambda^\ast_{k^\ast}}{\lambda_0 e^{-D^\ast_{k^\ast}}}
       \right]
\end{eqnarray}
where $k^\ast = \text{max}\{k\in\{1,\ldots,K\}\,:\,\Lambda^\ast_{k}\leq -\log U\}$.  In the case
where $C^\ast_{0k^\ast}=0$ instead we have:
\[
   T = \theta^\ast_{k^\ast} + \frac{-\log U - \Lambda^\ast_{k^\ast}}{\lambda_0 e^{-D^\ast_{k^\ast}}}
\]

Priors are needed for $\lambda_0$ and $w_0$.  Suitable priors are
\begin{eqnarray}
   \frac{1}{\gamma}\log\lambda_0 &\sim& \text{Normal}(0,\nu^{-2})\\
   \frac{w_0}{\gamma} &\sim& \text{Normal}(0,\nu^{-2}) 
\end{eqnarray}

## Samplers

In all cases
\begin{eqnarray}
   T_i \overset{\text{iid}}{\sim} f(\cdot|\lambda(\cdot|\Omega))
\end{eqnarray}
Some observations are fully observed ($t_i=T_i$), whereas others 
are censored: $t_i\leq T_i$.  The indicator $c_i=0$ if the observation 
is censored, $c_i=1$ if it is observed.  
Ordering the observations so that the first $n_0\leq n$ have $c_i=1$, 
the likelihood is
\[
   L(\Omega|{\bf t},{\bf c})
   = 
   f({\bf t}|\Omega,{\bf c})
   = 
   \left[\prod_{i=1}^{n_0} \lambda(t_i|\Omega) \right] 
   \exp\left(-\sum_{i=1}^n \Lambda(t_i|\Omega)\right)
\]
given parameters $\Omega$, with prior
\[
   f(\Omega)
\]
In what follows we propose parameter $\Omega^+$ from distribution
$q(\Omega^+|\Omega,{\bf t},{\bf c})$.  The proposal is accepted
with probability $\text{min}(1,r)$ where $r$ is the acceptance ratio
\begin{eqnarray}
  r &=& \frac{f({\bf t}|\Omega^+,{\bf c})}{f({\bf t}|\Omega,{\bf c})}
        \times
        \frac{f(\Omega^+)}{f(\Omega)}
        \times
        \frac{q(\Omega|\Omega^+,{\bf t},{\bf c})}{q(\Omega^+|\Omega,{\bf t},{\bf c})}\\
    &=& LR(\Omega^+|\Omega)
        \times
        \frac{f(\Omega^+)}{f(\Omega)}
        \times
        \frac{q(\Omega|\Omega^+,{\bf t},{\bf c})}{q(\Omega^+|\Omega,{\bf t},{\bf c})}
\end{eqnarray}
where the likelihood ratio is
\begin{eqnarray}
  LR(\Omega^+|\Omega)
  &=& \frac{f({\bf t}|\Omega^+,{\bf c})}{f({\bf t}|\Omega,{\bf c})}\\
  &=& \left[\prod_{i=1}^{n_0} \frac{\lambda(t_i|\Omega^+)}{\lambda(t_i|\Omega)} \right] 
   \exp\left(-\sum_{i=1}^n \left(\Lambda(t_i|\Omega^+)-\Lambda(t_i|\Omega)\right)\right)
\end{eqnarray}
If we prefer to make a proposal for the transformed parameter $\Psi(\Omega)$ then $r$ 
becomes
\begin{eqnarray}
  r &=& LR(\Omega^+|\Omega)
        \times
        \frac{f(\Omega^+)}{f(\Omega)}
        \times
        \frac{\left|\frac{\partial(\Psi)}{\partial(\Omega)}\right|}{
              \left|\frac{\partial(\Psi^+)}{\partial(\Omega^+)}\right|}
        \times
        \frac{q(\Psi|\Omega^+,{\bf t},{\bf c})}{q(\Psi^+|\Omega,{\bf t},{\bf c})}
\end{eqnarray}

In particular note that in the stick breaking algorithm, terminated after $K-1$ breaks: 
\begin{eqnarray}
   w_k &=& \gamma v_k \prod_{\ell < k} (1-v_\ell)\\
   w_K &=& \gamma-\sum_{\ell=1}^{K-1} w_\ell\\
       &=& \gamma\prod_{\ell=1}^{K-1} (1-v_\ell)\\
   \gamma &=& \sum_{k=1}^K w_k\\
   v_k &=& \frac{w_k}{\sum_{\ell=k}^K w_\ell}
\end{eqnarray}
and the Jacobian of the transformation between $(\gamma,v_1,\ldots,v_{K-1})$ and
$(w_1,\ldots,w_K)$ is
\begin{eqnarray}
   J &=& \left|\frac{\partial({\bf w})}{\partial(\gamma,{\bf v})}\right|\\
     &=& \prod_{\ell=1}^{K-1} \frac{w_\ell}{v_\ell}\\
     &=& \gamma^{K-1}\prod_{\ell=1}^{K-2} (1-v_\ell)^{K-k-1}
\end{eqnarray}


We will use log Normal:
\[
    q(y^+|y) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2} (y+-y)^2}
\]
the log Normal
\[
    q(y^+|y) = \frac{1}{y^+\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2} (\log y+-\log y)^2}
\]
and the logistic Normal
\[
    q(y^+|y) = \frac{1}{y^+(1-y^+)\sqrt{2\pi\sigma^2}}
               e^{-\frac{1}{2\sigma^2} (\text{logit}\,y+-\text{\logit}\,y)^2}
\]
as proposal distributions in Metropolis-Hastings updates.

### IFR case

#### Priors

Parameters: $\Omega=\{\eta, \gamma, \theta, {\bf v}\}$

Hazard rate function
\begin{eqnarray}
  \lambda(t|\Omega) &=& \lambda_0 + \gamma\sum_{k=1}^K \tilde{w}({\bf v}) I(\theta_k\leq t)\\
                    &=& \gamma\left(\eta + \sum_{k=1}^K \tilde{w}({\bf v}) I(\theta_k\leq t)\right)\\
                    &=& \gamma \tilde{\lambda}(t|\Omega\backslash\gamma)\\
  \Lambda(t|\Omega) &=& \lambda_0 t 
                        + \gamma \sum_{k=1}^K \tilde{w}_k({\bf v}) \text{max}(0,t-\theta_k)\\
                    &=& \gamma\left(\eta t 
                        + \sum_{k=1}^K \tilde{w}_k({\bf v}) \text{max}(0,t-\theta_k)\right)\\
                    &=& \gamma\tilde{\Lambda}(t|\Omega\backslash\gamma)
\end{eqnarray}

Priors:
\begin{eqnarray}
   \eta\equiv\frac{\lambda_0}{\gamma} &\sim& \text{Exp}(\nu)\\
   \gamma | \alpha,\beta &\sim& \text{Gamma}(\alpha,\beta)\\
   \theta_k | H_0(\cdot|\phi) &\sim& \text{Exp}(\phi)\\
   v_k | \alpha &\sim& \text{Beta}(1,\alpha)\\
   \alpha &\sim& \text{Gamma}(a_1,a_2)\\ 
   \beta &\sim& \text{Gamma}(b_1,b_2)\\ 
   \phi &\sim& \text{Gamma}(f_1,f_2)
\end{eqnarray}

Fixed values: $K$, $\nu$, $a_1$, $a_2$, $b_1$, $b_2$, $f_1$, $f_2$

Proposal parameters: $\sigma_\eta^2$, $\sigma_\theta^2$, $\sigma_v^2$, 
$\sigma_\alpha^2$, $\sigma_w^2$. 

#### $\eta$  

Full conditional:
\begin{eqnarray}
  f(\eta | - ) &\propto&
    f({\bf t}| - ) \times e^{-\nu\eta}
\end{eqnarray}
M-H update with a log Normal proposal $\text{logN}(\cdot|\eta,\sigma_\eta^2)$.

Proposal ratio:
\begin{eqnarray}
   r &=& LR \times \frac{\eta^+}{\eta} e^{-\nu(\eta^+-\eta)}\\
     &=& \left[\prod_{i=1}^{n_0}
               \frac{\lambda(t_i|\lambda_0^+,\gamma,\theta,{\bf v})}{
                     \lambda(t_i|\lambda_0,\gamma,\theta,{\bf v})}\right]
         \frac{\eta^+}{\eta}  
         e^{-(\eta^+-\eta)[\nu + \gamma\sum_{i=1}^n t_i]}\\
\log r &=& \sum_{i=1}^{n_0}
           \log\left(\frac{\lambda(t_i|\lambda_0^+,\gamma,\theta,{\bf v})}{
                      \lambda(t_i|\lambda_0,\gamma,\theta,{\bf v})}\right)\\
        && + \log\frac{\eta^+}{\eta} 
           -(\eta^+-\eta)\left[\nu + \gamma\sum_{i=1}^n t_i\right]
\end{eqnarray}

#### $\gamma$

Full conditional:
\begin{eqnarray}
  f(\gamma | - ) &\propto& 
            \left[\prod_{i=1}^{n_0} 
                  \lambda(t_i | \eta, \gamma, \mathbf{\theta}, \mathbf{v})\right]
            \exp\left[
               -\sum_{i=1}^n\Lambda(t_i | \eta,\gamma,\mathbf{\theta}, \mathbf{v})\right]\\
            &&\times \gamma^{\alpha-1}e^{\beta\gamma}\\
            &\propto& \gamma^{n_0} e^{-\gamma\sum_{i=1}^n \tilde{\Lambda}(t_i|\Omega\backslash\gamma)}
               \gamma^{\alpha-1}e^{\beta\gamma}\\
            &\propto& \gamma^{\alpha+n_0-1} e^{-\gamma(\beta + \sum_{i=1}^n \tilde{\Lambda}(t_i|\Omega\backslash\gamma))}
\end{eqnarray}
Gibbs update:
\[
   \gamma | - \sim \text{Gamma}\left(\alpha+n_0, 
                 \beta+\sum_{i=1}^n \tilde{\Lambda}(t_i|\Omega\backslash\gamma)\right)
\]



#### $\theta_k$

Full conditional:
\begin{eqnarray}
  f(\theta_k | - ) &\propto& 
            \left[\prod_{i=1}^{n_0} \lambda(t_i | \Omega)\right]
            \exp\left[-\sum_{i=1}^n\Lambda(t_i | \Omega)\right]\\
            &&\times \exp(-\phi\theta_k) 
  \end{eqnarray}
Proposal: Select $k\sim\text{Cat}(\tilde{\mathbf{w}}(\mathbf{v}))$ and then propose 
$\theta^+_k$ by a log Normal random walk Metropolis step: 
$\log\theta_k^+ | \log\theta_k, \sigma_\theta^2 \sim N(\log\theta_k, \sigma_\theta^2)$
\begin{eqnarray}
  q(k, \theta^+_k | \theta_k, \mathbf{v}) 
                    &\propto& \frac{\tilde{w}_k}{\theta^+_k} 
                    e^{-(\log\theta^+_k-\log\theta_k)^2/(2\sigma_\theta^2)}
\end{eqnarray}
Proposal ratio:
\begin{eqnarray}
  r &=& \frac{f(\theta^+_k|-)}{f(\theta_k|-)} 
        \frac{q(k, \theta_k | \theta^+_k, \mathbf{v})}{q(k, \theta^+_k | \theta_k, \mathbf{v})}\\
    &=& LR(\Omega^+|\Omega) \times e^{-\phi(\theta^+_k-\theta_k)}\frac{\theta^+_k}{\theta_k}\\
  \log r &=& \log LR(\Omega^+|\Omega) -\phi(\theta^+_k-\theta_k) + \log\theta^+_k-\log\theta_k
\end{eqnarray}
  
#### $v_k$
  
Full conditional:
\begin{eqnarray}
  f(v_k | - ) &\propto& 
            \left[\prod_{i=1}^{n_0} \lambda(t_i | \mathbf{\theta}, \mathbf{v})\right]
            \exp\left[-\sum_{i=1}^n\Lambda(t_i | \mathbf{\theta}, \mathbf{v})\right]\\
            &&\times (1-v_k)^{\alpha-1}
\end{eqnarray}
Proposal: Select $k\sim\text{Cat}(\tilde{\mathbf{w}}(\mathbf{v})/(1-\tilde{w}_K))$ - only
select $k$ in $\{1,\ldots,K-1\}$.  Then propose 
$v^+_k$ by a logistic Normal random walk Metropolis step: 
$\text{logit}v_k^+ | \text{logit}\,v_k, \sigma_v^2 \sim N(\text{logit}\,v_k, \sigma_v^2)$
\begin{eqnarray}
  q(k, v^+_k | v_k, \mathbf{v}_{-k}) 
                    &\propto& 
                    \frac{\tilde{w}_k(\mathbf{v})}{(1-\tilde{w}_K(\mathbf{v}))v^+_k(1-v^+_k)} 
                    e^{-(\text{logit}\,v^+_k-\text{logit}\,v_k)^2/(2\sigma_v^2)}\\
                    &\propto& \frac{v_k}{(1-\tilde{w}_K(\mathbf{v}))v^+_k(1-v^+_k)} 
                    e^{-(\text{logit}\,v^+_k-\text{logit}\,v_k)^2/(2\sigma_v^2)}\\
\end{eqnarray}
Proposal ratio:
\begin{eqnarray}
  r &=& \frac{f(v^+_k|-)}{f(v_k|-)} 
        \frac{q(k, v_k | v^+_k, \mathbf{v}_{-k})}{q(k, v^+_k | v_k, \mathbf{v}_{-k})}\\
    &=& LR(\Omega^+|\Omega)      
         \left(\frac{v_k^+}{v_k}\right)^2\left(\frac{1-v_k^+}{1-v_k}\right)^{\alpha}
         \frac{1-\tilde{w}_K}{1-\tilde{w}^+_K}\\
  \log r &=& \log LR(\Omega^+|\Omega)\\        
     &&+\alpha\log\left(\frac{1-v_k^+}{1-v_k}\right)+2\log\left(\frac{v_k^+}{v_k}\right)\\
     &&+\log\left(\frac{1-\tilde{w}_K}{1-\tilde{w}^+_K}\right)
\end{eqnarray}
where $\tilde{w}_K({\bf v})=1-\sum_{k=1}^{K-1} \tilde{w}_\ell({\bf v})$.

NB: if we update all components in sequence from $k=1,\ldots,K$ (without random selection), we 
don't have the selection probabilties in the proposal ratio.  Therefore it becomes:
\begin{eqnarray}
  \log r &=& \log LR(\Omega^+|\Omega)\\        
     &&+\alpha\log\left(\frac{1-v_k^+}{1-v_k}\right)+\log\left(\frac{v_k^+}{v_k}\right)
\end{eqnarray}
where the final term has a multiplier of 1 not 2.
  
#### $\alpha$

Full conditional:
\begin{eqnarray}
  f(\alpha | - ) &\propto& 
            f(\gamma | \alpha,\beta) f({\bf v}|\alpha) f(\alpha)\\
            &\propto&
            \frac{\beta^\alpha}{\Gamma(\alpha)} \gamma^{\alpha-1}
            \left[\prod_{k=1}^{K-1}\alpha (1-v_k)^{\alpha-1} \right]
            \alpha^{a_1-1} e^{-a_2\alpha}\\
            &\propto&
            \frac{1}{\Gamma(\alpha)} 
            \alpha^{a_1+K-2} e^{-\alpha(a_2-\log\tilde{w}_K-\log\beta\gamma)}
\end{eqnarray}
Proposal: Propose 
$\alpha^+$ by a log Normal random walk Metropolis step: 
$\text{log}\alpha^+ | \alpha, \sigma_\alpha^2 \sim N(\log\alpha, \sigma_\alpha^2)$
\begin{eqnarray}
  q(\alpha^+ | \alpha) 
                    &\propto& \frac{1}{\alpha^+} 
                    e^{-(\log\alpha^+-\log\alpha)^2/(2\sigma_\alpha^2)}
\end{eqnarray}
Proposal ratio:
\begin{eqnarray}
  r &=& \frac{\Gamma(\alpha)}{\Gamma(\alpha^+)}
        \left(\frac{\alpha^+}{\alpha}\right)^{a_1+K-1}
        e^{-(\alpha^+-\alpha)[a_2-\log\tilde{w}_K-\log\beta\gamma]}\\
\log r &=& \log\left(\frac{\Gamma(\alpha)}{\Gamma(\alpha^+)}\right)
        + (a_1+K-1)\log\frac{\alpha^+}{\alpha}\\
       &&  -(\alpha^+-\alpha)[a_2-\log\tilde{w}_K-\log\beta\gamma]
\end{eqnarray}

#### $\beta$

Full conditional:
\begin{eqnarray}
  f(\beta | - ) &\propto& 
            f(\gamma | \alpha,\beta) f(\beta)\\
            &\propto&
            \beta^\alpha e^{-\beta\gamma} 
            \beta^{b_1-1} e^{-b_2\beta}\\
            &\propto&
            \beta^{b_1+\alpha-1} e^{-\beta(b_2+\gamma)}\\
  \beta | - &\sim& \text{Gamma}(b_1+\alpha, b_2+\gamma)
\end{eqnarray}
Gibbs update

#### $\phi$

Full conditional:
\begin{eqnarray}
  f(\phi | - ) &\propto& 
            \left[\prod_{k=1}^K f(\theta_k|\phi)\right] f(\phi)\\
            &\propto&
            \phi^K e^{-\phi\sum_{k=1}^K\theta_k} 
            \phi^{f_1-1} e^{-f_2\phi}\\
            &\propto&
            \phi^{f_1+K-1} e^{-\phi(f_2+\sum_{k=1}^K\theta_k)}\\
  \phi | - &\sim& \text{Gamma}(f_1+K, f_2+\sum_{k=1}^K\theta_k)
\end{eqnarray}
Gibbs update

#### $w_k$

As an alternative to the $\gamma$ and $v_k$ updates, we can update $w_k=\gamma v_k$ directly.

Proposal: Select $k\sim\text{Cat}(\tilde{\mathbf{w}}(\mathbf{v}))$ and then propose 
$w^+_k$ by a log Normal random walk Metropolis step: 
$\log w_k^+ | \log w_k, \sigma_w^2 \sim N(\log w_k, \sigma_w^2)$
\begin{eqnarray}
  q(k, w^+_k | w_k, \gamma) 
                    &\propto& \frac{\tilde{w}_k}{w^+_k} 
                    e^{-(\log w^+_k-\log w_k)^2/(2\sigma_w^2)}\\
                    &\propto& \frac{w_k}{\gamma w^+_k} 
                    e^{-(\log w^+_k-\log w_k)^2/(2\sigma_w^2)}
\end{eqnarray}
Compute
\begin{eqnarray}
  \gamma^+ &=& \sum_{\ell=1}^K w^+_\ell\\
  \tilde{w}^+_\ell &=& \frac{w^+_\ell}{\gamma^+}
  v^+_\ell &=& \frac{\tilde{w}^+_\ell}{\sum_{j=\ell}^K \tilde{w}_j}\\
  \lambda_0^+ &=& \gamma^+\eta\\
\end{eqnarray}


Proposal ratio:
\begin{eqnarray}
  r &=& \frac{f({\bf t}|\Omega^+)}{f({\bf t}|\Omega)}
        \times
        \frac{f(\eta)f(\gamma^+)f(\theta)f({\bf v}^+)}{
              f(\eta)f(\gamma)f(\theta)f({\bf v})}
        \times
        \frac{\left|\frac{\partial({\bf w})}{\partial(\gamma,{\bf v})}\right|}{
              \left|\frac{\partial({\bf w}^+)}{\partial(\gamma^+,{\bf v}^+)}\right|}
        \times
        \frac{q(k,w_k|w^+_k,\gamma)}{q(k,w^+k|w_k,\gamma)}\\
    &=& LR(\Omega^+|\Omega)
        \left(\frac{\gamma^+}{\gamma}\right)^{\alpha-2}
        \left(\frac{w_k^+}{w_k}\right)
        e^{-\beta(\gamma^+-\gamma)}
        \left[\prod_{\ell=1}^{K-1} 
        \frac{v_\ell^+}{v_\ell}\left(\frac{1-v^+_\ell}{1-v_\ell}\right)^{\alpha-1}\right]\\
\log r &=& \log LR(\Omega^+|\Omega)\\
       && + (\alpha-2)\log\left(\frac{\gamma^+}{\gamma}\right)
          + \log\left(\frac{w_k^+}{w_k}\right)\\
       && - \beta(\gamma^+-\gamma)
          + \sum_{\ell=1}^{K-1} \left[
            \log\frac{v^+_\ell}{v_\ell} + (\alpha-1)\log\frac{1-v^+_\ell}{1-v_\ell}
            \right]
\end{eqnarray}

NB: if we update all components in sequence from $k=1,\ldots,K$ (without random selection), we 
don't have the selection probabilties in the proposal ratio.  Therefore it becomes:
\begin{eqnarray}
\log r &=& \log LR(\Omega^+|\Omega)\\
       && + (\alpha-1)\log\left(\frac{\gamma^+}{\gamma}\right)
       && - \beta(\gamma^+-\gamma)
          + \sum_{\ell=1}^{K-1} \left[
            \log\frac{v^+_\ell}{v_\ell} + (\alpha-1)\log\frac{1-v^+_\ell}{1-v_\ell}
            \right]
\end{eqnarray}

# Computing WAIC

Following @Watanabe:2010: Given a set of independent observations 
$\{y_i\}_{i=1}^n$ that are conditionally independent 
given the parameter $\theta$ - i.e. having probability distribution 
$f(y_i|\theta)$ and given some prior $\pi(\theta)$, we can generate 
a set of $T$ draws $\theta_t$ from the posterior distribution 
$f(\theta|{\bf y})$.

The WAIC can then be calculated:
\begin{eqnarray}
   \text{WAIC} 
   &=& -2 \sum_{i=1}^n \log\left(\text{mean}\left[\{f(y_i|\theta_t)\}_{t=1}^T\right]\right)\\
   &&+2 \sum_{i=1}^n \text{var}\left[\{\log(f(y_i|\theta_t))\}_{i=1}^n\right]
\end{eqnarray}
When comparing models we prefer the model with the lowest WAIC.

# Bibliography